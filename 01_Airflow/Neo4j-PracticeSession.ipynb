{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.delete_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Connection to Neo4j DB!!\n"
     ]
    }
   ],
   "source": [
    "from py2neo import Graph\n",
    "try:\n",
    "    graph = Graph(\"bolt://neo:11002\")\n",
    "except:\n",
    "    print(\"Error Connection to Neo4j DB!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='tryout.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "metadata  = []\n",
    "\n",
    "lines = 50    # 50 for testing\n",
    "\n",
    "with open(file, 'r') as f:\n",
    "    \n",
    "    for line in f:\n",
    "        metadata.append(json.loads(line))\n",
    "        lines -= 1\n",
    "        if lines == 0: break\n",
    "            \n",
    "df = pd.DataFrame(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_944/2034437708.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mtöödeldud_andmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menrich_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_944/2034437708.py\u001b[0m in \u001b[0;36menrich_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mmydivs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"span\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"primary-subject\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m                     \u001b[0;34m\"Couldn't find a tree builder with the features you \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                     \u001b[0;34m\"requested: %s. Do you need to install a parser library?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                     % \",\".join(features))\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# At this point either we have a TreeBuilder instance in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "#!pip install crossrefapi\n",
    "from crossref.restful import Works\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def enrich_data(df):\n",
    "  works = Works()\n",
    "\n",
    "  df[\"type\"] = np.nan\n",
    "  df[\"referenced\"] = np.nan\n",
    "  df[\"subject\"] = np.nan\n",
    "  df[\"ISSN\"] = np.nan\n",
    "  df[\"publisher\"] = np.nan\n",
    "  df[\"venue\"] = np.nan\n",
    "\n",
    "  for i in range(len(df)):\n",
    "    authors = df[\"authors\"][i]\n",
    "    publication = df[\"title\"][i]\n",
    "    try:\n",
    "      w1 = works.query(publication, author = authors)\n",
    "      for j, member in enumerate(w1):\n",
    "        if j == 0:\n",
    "          df.iloc[i,6] = member[\"DOI\"]\n",
    "          try:\n",
    "            df[\"type\"][i] = member[\"type\"]\n",
    "          except:\n",
    "            continue\n",
    "          try:\n",
    "            df[\"referenced\"][i] = member[\"reference-count\"]\n",
    "          except:\n",
    "            continue\n",
    "          try:\n",
    "            if type(member[\"subject\"]) == list:\n",
    "              df[\"subject\"][i] = member[\"subject\"][0]\n",
    "            else:\n",
    "              df[\"subject\"][i] = member[\"subject\"]\n",
    "          except:\n",
    "            continue\n",
    "          try:\n",
    "            if type(member[\"ISSN\"]) == list:\n",
    "              df[\"ISSN\"][i] = member[\"ISSN\"][0]\n",
    "            else:\n",
    "              df[\"ISSN\"][i] = member[\"ISSN\"]\n",
    "          except:\n",
    "            continue\n",
    "          try:\n",
    "            df[\"publisher\"][i] = member[\"publisher\"]\n",
    "          except:\n",
    "            continue\n",
    "          try:\n",
    "            df[\"venue\"][i] = member[\"container-title\"][0]\n",
    "          except:\n",
    "            continue\n",
    "        else:\n",
    "          break\n",
    "    except:\n",
    "      continue\n",
    "\n",
    "  df[\"category_full\"] = \"NA\"\n",
    "\n",
    "  for i in range(len(df)):\n",
    "    id = df.iloc[i,0]\n",
    "\n",
    "    url = 'https://arxiv.org/abs/' + str(id)\n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "    mydivs = soup.find(\"span\", {\"class\": \"primary-subject\"})\n",
    "\n",
    "    df[\"category_full\"][i] = mydivs.text\n",
    "  return df\n",
    "\n",
    "töödeldud_andmed = enrich_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_list(line):\n",
    "    # Cleans author dataframe column, creating a list of authors in the row.\n",
    "    return [e[1] + ' ' + e[0] for e in line]\n",
    "\n",
    "\n",
    "def get_category_list(line):\n",
    "    # Cleans category dataframe column, creating a list of categories in the row.\n",
    "    return list(line.split(\" \"))\n",
    "\n",
    "\n",
    "df['cleaned_authors_list'] = df['authors_parsed'].map(get_author_list)\n",
    "df['category_list'] = df['categories'].map(get_category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def add_categories(categories):\n",
    "    # Adds category nodes to the Neo4j graph.\n",
    "    query = '''\n",
    "            UNWIND $rows AS row\n",
    "            MERGE (c:Category {category: row.category})\n",
    "            RETURN count(*) as total\n",
    "            '''\n",
    "    return graph.run(query, parameters = {'rows':categories.to_dict('records')})\n",
    "\n",
    "\n",
    "def add_authors(rows, batch_size=10000):\n",
    "    # Adds author nodes to the Neo4j graph as a batch job.\n",
    "    query = '''\n",
    "            UNWIND $rows AS row\n",
    "            MERGE (:Author {name: row.author})\n",
    "            RETURN count(*) as total\n",
    "            '''\n",
    "    return insert_data(query, rows, batch_size)\n",
    "\n",
    "\n",
    "def add_publishers(publishers):\n",
    "    #Adds publisher nodes to the Neo4j graph\n",
    "    query= '''\n",
    "            UNWIND $rows AS row\n",
    "            MERGE (j:Publisher {publisher: row.publisher})\n",
    "            RETURN count(*) as total\n",
    "            '''\n",
    "    return graph.run(query, parameters = {\"rows\":publishers.to_dict(\"records\")})\n",
    "\n",
    "def add_venues(venues):\n",
    "    #Adds publisher nodes to the Neo4j graph\n",
    "    query= '''\n",
    "            UNWIND $rows AS row\n",
    "            MERGE (j:Venue {venue: row.venue})\n",
    "            RETURN count(*) as total\n",
    "            '''\n",
    "    return graph.run(query, parameters = {\"rows\":venues.to_dict(\"records\")})\n",
    "\n",
    "\n",
    "def insert_data(query, rows, batch_size = 10000):\n",
    "    # Function to handle the updating the Neo4j database in batch mode.\n",
    "    \n",
    "    total = 0\n",
    "    batch = 0\n",
    "    start = time.time()\n",
    "    result = None\n",
    "    \n",
    "    while batch * batch_size < len(rows):\n",
    "\n",
    "        res = graph.run(query,parameters = {'rows': rows[batch*batch_size:(batch+1)*batch_size].to_dict('records')})\n",
    "        batch += 1\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_papers(rows, batch_size=5000):\n",
    "   # Adds paper nodes and (:Author)--(:Paper) and \n",
    "   # (:Paper)--(:Category) relationships to the Neo4j graph as a \n",
    "   # batch job.\n",
    " \n",
    "   query = '''\n",
    "   UNWIND $rows as row\n",
    "   MERGE (p:Paper {id:row.id}) ON CREATE SET p.title = row.title\n",
    " \n",
    "   // connect categories\n",
    "   WITH row, p\n",
    "   UNWIND row.category_list AS category_name\n",
    "   MATCH (c:Category {category: category_name})\n",
    "   MERGE (p)-[:IN_CATEGORY]->(c)\n",
    " \n",
    "   // connect authors\n",
    "   WITH distinct row, p // reduce cardinality\n",
    "   UNWIND row.cleaned_authors_list AS author\n",
    "   MATCH (a:Author {name: author})\n",
    "   MERGE (a)-[:AUTHORED]->(p)\n",
    "\n",
    "   //connect co-authors\n",
    "   WITH row, p\n",
    "   MATCH (a1:Author)-[:AUTHORED]-(:Paper)-[:AUTHORED]-(a2:Author) \n",
    "   MERGE (a1)-[:CO_AUTHOR]-(a2)\n",
    "   \n",
    "   // connect publishers\n",
    "   WITH row, p\n",
    "   UNWIND row.publisher AS publisher\n",
    "   MATCH (f:Publisher {publisher: publisher})\n",
    "   MERGE (p)-[:PUBLISHED_IN]->(f)\n",
    "   \n",
    "   // connect venues\n",
    "   WITH row, p\n",
    "   UNWIND row.venue AS venue\n",
    "   MATCH (v:Venue {venue: venue})\n",
    "   MERGE (p)-[:VENUE]->(v)\n",
    "\n",
    "   RETURN count(distinct p) as total\n",
    "   '''\n",
    "\n",
    "\n",
    " \n",
    "   return insert_data(query, rows, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "categories = pd.DataFrame(df[['category_list']])\n",
    "categories.rename(columns={'category_list':'category'},\n",
    "                  inplace=True)\n",
    "\n",
    "categories = categories.explode('category').drop_duplicates(subset=['category'])\n",
    "print(type(categories.to_dict(\"records\")))\n",
    "\n",
    "authors = pd.DataFrame(df[['cleaned_authors_list']])\n",
    "authors.rename(columns={'cleaned_authors_list':'author'},\n",
    "               inplace=True)\n",
    "authors=authors.explode('author').drop_duplicates(subset=['author'])\n",
    "\n",
    "journals= pd.DataFrame(df[['journal-ref']])\n",
    "\n",
    "journals=journals.explode('journal-ref').drop_duplicates(subset=['journal-ref']).dropna(axis=0)\n",
    "\n",
    "publishers = pd.DataFrame(df[[\"publisher\"]])\n",
    "publishers = publishers.drop_duplicates(subset = [\"publisher\"]).dropna(axis = 0)\n",
    "\n",
    "venues = pd.DataFrame(df[[\"venue\"]])\n",
    "venues = venues.drop_duplicates(subset = [\"venue\"]).dropna(axis = 0)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "add_categories(categories)\n",
    "add_venues(venues)\n",
    "add_publishers(publishers)\n",
    "add_authors(authors)\n",
    "add_papers(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
